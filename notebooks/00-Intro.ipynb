{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd3a42efa31ff3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Artificial Intelligence Primer\n",
    "\n",
    "Welcome! This project offers a quick view and practical examples for AI, avoiding the complex installation setup.\n",
    "\n",
    "We will start will the concepts of Artificial Intelligence, Machine Learning, Deep Learning, and AI frameworks. Then we will explore each architecture in increasing level of complexity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f4470-8911-4a9a-8f0a-d1e700eb530f",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "This is a Jupyter notebook. Notice that we can have both prose, like this explanation, and code, like the cell below. To execute the cell below, select it and press the ▶️ (*Run this cell and Advance*) icon in the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88800966-4135-459e-b57c-ba36013fa0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f3f63-3ec6-4ee3-98f0-34f8b0537f7f",
   "metadata": {},
   "source": [
    "The shortcut for execution is `Shift` + `Enter`.\n",
    "\n",
    "Optionally, you can press the ⏩ (*Restart the kernel and run all cells*) icon to execute the whole notebook at once. Since this is a learning and not a production notebook, we recommend you step one cell at a time instead.\n",
    "\n",
    "Notice that you can execute cells in any order, or multiple times. The history of cell execution is labeled to the left of each cell, like \\[1\\] indicating this is the first cell that was executed. Most notebooks expect a linear progression, but you can re-run any cell at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039808fa-191d-4b99-bc1a-ba5a801f6e6c",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "You probably have heard AI-related concepts before, but it's good to review their hierarchy.\n",
    "\n",
    "![](../media/intro/A-comparative-view-of-AI-machine-learning-deep-learning-and-generative-AI-source.png \"Miltiadis D. Lytras\")\n",
    "\n",
    "To allow work in AI, we have different tools:\n",
    "\n",
    "- **Python**: Python is the primary language we'll be using for AI.\n",
    "- **PyTorch & torchvision**: PyTorch is an open-source machine learning library, and torchvision offers datasets and models for computer vision.\n",
    "- **Jupyter Notebook**: The interactive environment where this tutorial is presented.\n",
    "- **NumPy**: A library for numerical operations in Python.\n",
    "- **scikit-learn**: Machine learning library in Python. We'll use it for performance metrics.\n",
    "- **Seaborn & Matplotlib**: Visualization libraries in Python.\n",
    "- **CUDA** (Optional): If you have a compatible NVIDIA GPU, you can install CUDA for GPU acceleration with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11673206-2fdf-4cb2-916b-8f84775840ed",
   "metadata": {},
   "source": [
    "# Deep Learning Summary\n",
    "\n",
    "Below is a summary of the main concepts of Deep Learning. It's very condensed but it'll give you the reasoning behind each concept. For more in-depth information, you can search online, there's plenty of material there, although it's scattered. Subsequent notebooks will have you implement these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00dd89-82d5-466b-804c-d71e6f63e174",
   "metadata": {},
   "source": [
    "## Neural Networks and Deep Learning\n",
    "\n",
    "\n",
    "Traditional learning algorithms do not scale with the amount of data. So Deep Learning takes advantage of large amounts of labeled data to increase performance of analyses, predictions, and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786a4a0-65bb-4b43-80c1-8bf7b828c6f0",
   "metadata": {},
   "source": [
    "### Neural Network basics\n",
    "\n",
    "You probably recall from middle school that a linear function has the form `y = mx + b`. The basic neuron in a Neural Network also consists of a weight `w` and a bias `b`. This allows a neuron to change parameters to best fit a set of data.\n",
    "\n",
    "![](../media/intro/weights_and_biases.png \"@theDrewDag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de76a8-9263-4448-b605-497adf7f5390",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "We are looking for the best equation to describe a set of data, so we use linear regression to best fit the parameters to fit our inputs. In the graphs above, we would use (x,y) pairs as our data. For something like a cat image classifier, we would use the value of each RGB pixel as our `x` input, and our `y` output would be whether the picture is of a cat or not (label). Because the answer is either True of False, this is a Binary CLassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634b3db-45f6-4993-91e5-7169170f1e9a",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "How do we know is our system is doing a good job? We use the loss function. It measures how far away our prediction is from the expected result (label). This information is used to update our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29beb5-4327-4bc4-bd82-e4de1328ae2b",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "How do we update the numbers in our weight and biases parameters? We use derivatives on the cost function and a lot of math. We want to cost function to be minimized, so we use gradient descent to find the lowest cost point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd7904-6867-4587-a7ec-2a875bf1d28b",
   "metadata": {},
   "source": [
    "#### Activation function\n",
    "\n",
    "As it turns out, no matter how many neurons we use, if they are all linear functions, the end result if a linear function as well! This severely hinders our options. An activation function is non-linear, essentially allows us to say \"behave like this linear function, but only after a threshold.\" It's like adding a digital switch to an analog circuit, and it makes possible Neural Networks. A common activation function is the REctified Linuear Unit, ReLU.\n",
    "\n",
    "<img src=\"../media/intro/ReLU.png\" alt=\"ReLU\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b4918-197e-4bf4-9393-68bdb35e2840",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "With each neuron having parameters (weight, bias) and an activation function (e.g, ReLU), we can make a Neural Network, NN. A shallow NN has one internal layer, and it's fully connected.\n",
    "\n",
    "Displayed is also a deep NN has multiple hidden layers.\n",
    "\n",
    "![](../media/intro/Neural_networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72292929-becf-4f13-8181-84f2e0893f24",
   "metadata": {},
   "source": [
    "#### Layers\n",
    "\n",
    "\n",
    "- The input layer is generally a flattened version of your data. For example, an linear array of each R, G, and B, value for each pixel in an image.\n",
    "- The hidden layers are your Neural Network Layers\n",
    "- The output layer corresponds to how many outputs you need. One for binary classification, multiple for categorization, etc.\n",
    "\n",
    "Do you see the ReLU activation function above? It's unbounded and generally doesn't work for outputs, since we want bounded outputs. For example, output probabilities are between 0 and 1. We generally use the sigmoid function for the output layer instead.\n",
    "\n",
    "<img src=\"../media/intro/Sigmoid.png\" alt=\"Sigmoid\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3b1b7-027b-4dfd-9269-9d326a059910",
   "metadata": {},
   "source": [
    "#### Multi-class Classification\n",
    "\n",
    "When having multiple dependent outputs, for example in categorization, it's useful to use Softmax regression. It's an output layer showing the probabilities for an input to be in any category, with probabilities adding up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb3719-07ef-47ad-a16b-0051a8d6e0ec",
   "metadata": {},
   "source": [
    "### Forward and Backward Propagation\n",
    "\n",
    "We train NNs from scratch. We initialize our model with random weights and zero biases, and pass our training data through the NN to get predicted outputs. These predictions will probably suck. But from the training data we know what they *should* be, so we calculate the loss function on that. And then we use these values to backpropagate changes into the NN, updating parameter values for weights and biases. And guess what? Our predictions would be better next time.\n",
    "\n",
    "So we:\n",
    "\n",
    "- Do a forward pass\n",
    "- Calculate loss\n",
    "- Do a backward pass\n",
    "- Update parameters\n",
    "- Repeat multiple times (epochs) until we get close to expected results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fb90e-2e08-4ed4-9cec-3bdddac51308",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "To tune our NN, we need to choose hyperparamaters. These are manually set variables that control the training process of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada5b2a-cfae-4136-a258-dad81dd4b610",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "After we get the gradients from our loss function, we update the parameters by a fraction of the gradient to prevent overshoot. This fraction is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f98a3-2d1f-446d-ba8a-0c215737d403",
   "metadata": {},
   "source": [
    "### Train / Dev / Test\n",
    "\n",
    "You have a labeled dataset, but your model should work on new data as well. So, we split datasets into training set, development set, and test set.\n",
    "\n",
    "- Train: data use to train the model.\n",
    "- Dev: data to use for hyperparameter tuning. Matches production data (e.g: traffic pictures from a specific webcam)\n",
    "- Test: This set is never seen during training, and is only used to determine accuracy. Matches production data.\n",
    "\n",
    "The split between sets is a hyperparameter. Recommended for small datasets: 80%, 10%, 10%. For large datasets (> 1 million): 98%, 1%, 1% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b433fa-2e6e-4f60-b121-4919aba569a0",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Many times we overfit our data to our training set, and it doesn't perform well on our test set or real world data. To avoid that, we use regularization. Examples:\n",
    "\n",
    "- **Normalizing inputs**: NNs perform better with input values close to zero.\n",
    "- **Dropout**: If random neurons are ignored every now and then, the end result is not fixated on any specific feature.\n",
    "- **Data augmentation**: Modify your existing data, to expand your training set. For example, rotating/scaling/cropping your images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba001e8d-96ac-4085-8b93-109ed874d07f",
   "metadata": {},
   "source": [
    "### Optimization Algorithms\n",
    "\n",
    "We want to optimize our models to converge to a minimum loss quickly. For that we:\n",
    "\n",
    "- Use mini batches of training data, instead of waiting for the whole set to process.\n",
    "- Include momentum when doing gradient descent (RMSprop).\n",
    "- Include acceleration when doing gradient descent (Adam).\n",
    "- Learning Rate decay: Start fast learning, then converge slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d080afb-c452-47a9-86eb-12b9686adc6d",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "When selecting hyperparameters, remember that the scale for the values is usually logarithmic, not linear.\n",
    "\n",
    "When tuning, work on a single parameter at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0edd1-09a4-44ee-9918-26a565d281b5",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision. They are specifically designed to recognize visual patterns directly from pixel images with minimal preprocessing. CNNs are hierarchical models where neurons in one layer connect to neurons in the next layer in a limited fashion, somewhat like the receptive field in human vision.\n",
    "\n",
    "A typical CNN architecture consists of:\n",
    "\n",
    "- **Convolutional Layers**: Apply convolution operation on the input layer to detect features.\n",
    "- **Activation Layers**: Introduce non-linearity to the model (typically ReLU).\n",
    "- **Pooling Layers**: Perform down-sampling operations to reduce dimensionality.\n",
    "- **Fully Connected Layers**: After several convolutional and pooling layers, the high-level reasoning in the neural network happens via fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581e81f-43ac-4c68-98a4-1d349ebeb53b",
   "metadata": {},
   "source": [
    "### Residual Networks (ResNet)\n",
    "\n",
    "For most networks, a large number of inputs (e.g: pixels) get monotonically reduced to a small number of outputs (e.g: class). Input information is lost in deeper layers. With residual networks, or ResNets, parts of previous layers are copied, or 'skipped' to deeper layers.\n",
    "\n",
    "A useful trait of ResNets is that the offer transfer learning. Deep ResNets are trained on millions of images to classify 1000 different classes (e.g: ResNet18). That means most of the layers can already detect useful image features.\n",
    "\n",
    "#### Fine-tuning\n",
    "\n",
    "Given that ResNets can already detect useful features, we use fine-tuning to achieve transfer learning. We replace the existing output layer (or optionally, additional end layers) to fit our model outputs (for example, classifying between diferent types of apples). Training is fast since it only happens in the last layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76165866-855d-4283-8e25-6d5bdd7e1666",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "You've learned about a lot of Deep Learning concepts. It's time to see them in action.\n",
    "\n",
    "The notebooks following this one will have you explore PyTorch, Datasets, Neural Networks, Regularization, CNNs, and ResNets. Click on them in the navigation panel and run the cells as you progress through the material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bb342-2d38-45db-bf4a-028d2bcaf826",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [pytorch-fashionMNIST-tutorial](https://github.com/junaidaliop/pytorch-fashionMNIST-tutorial/blob/main/pytorch_fashion_mnist_tutorial.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
