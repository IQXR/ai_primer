{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd3a42efa31ff3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Intro - Artificial Intelligence Primer\n",
    "\n",
    "Welcome! This project offers a quick view and practical examples for AI, avoiding the complex installation setup.\n",
    "\n",
    "We will start will the concepts of Artificial Intelligence, Machine Learning, Deep Learning, and AI frameworks. Then we will explore each architecture in increasing level of complexity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039808fa-191d-4b99-bc1a-ba5a801f6e6c",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "You probably have heard AI-related concepts before, but it's good to review their hierarchy.\n",
    "\n",
    "![](../media/intro/A-comparative-view-of-AI-machine-learning-deep-learning-and-generative-AI-source.png \"Miltiadis D. Lytras\")\n",
    "\n",
    "To allow work in AI, we have different tools:\n",
    "\n",
    "- **Python**: Python is the primary programming language we'll be using for AI.\n",
    "- **PyTorch & torchvision**: PyTorch is an open-source machine learning library, and torchvision offers datasets and models for computer vision.\n",
    "- **Jupyter Notebook**: The interactive environment where this tutorial is presented.\n",
    "- **NumPy**: A library for numerical operations in Python.\n",
    "- **scikit-learn**: Machine learning library in Python. We'll use it for performance metrics.\n",
    "- **Seaborn & Matplotlib**: Visualization libraries in Python.\n",
    "- **CUDA** (Optional): If you have a compatible NVIDIA GPU, you can install CUDA for GPU acceleration with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11673206-2fdf-4cb2-916b-8f84775840ed",
   "metadata": {},
   "source": [
    "# Deep Learning Summary\n",
    "\n",
    "Below is a summary of the main concepts of Deep Learning. It's very condensed but it'll give you the reasoning behind each concept. For more in-depth information, you can search online, there's plenty of material there, although it's scattered. Subsequent notebooks will have you implement these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00dd89-82d5-466b-804c-d71e6f63e174",
   "metadata": {},
   "source": [
    "## Neural Networks and Deep Learning\n",
    "\n",
    "\n",
    "Traditional learning algorithms do not scale with the amount of data. So Deep Learning takes advantage of large amounts of labeled data to increase performance of analyses, predictions, and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786a4a0-65bb-4b43-80c1-8bf7b828c6f0",
   "metadata": {},
   "source": [
    "### Neural Network basics\n",
    "\n",
    "You probably recall from middle school that a linear function has the form `y = mx + b`. The basic neuron in a Neural Network also consists of a weight `w` and a bias `b`. This allows a neuron to change parameters to best fit a set of data.\n",
    "\n",
    "![](../media/intro/weights_and_biases.png \"@theDrewDag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de76a8-9263-4448-b605-497adf7f5390",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "We are looking for an equation to describe a set of data, so we use linear regression to fit the parameters to best fit our inputs. In the graphs above, we would use (x,y) pairs as our data. \n",
    "\n",
    "For something like a cat image classifier, we would use the value of each RGB color pixel as our `x` input, and our `y` output would be whether the picture is of a cat or not (its label). Since the linear regression classifies the variables, it becomes a Logistic regression. Because the answer is either True of False, this is a Binary Classifier.\n",
    "\n",
    "Binary Classifiers also apply to either-one-or-the-other class, like below.\n",
    "\n",
    "![](../media/intro/Logistic_regression.jpg \"AcademicianHelp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634b3db-45f6-4993-91e5-7169170f1e9a",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "How do we know is our system is doing a good job? We use the loss function. It measures how far away our prediction is from the expected result (label). This information is used to update our parameters. We usually use cross entropy loss in AI.\n",
    "\n",
    "<img src=\"../media/intro/Loss_function.jpg\" alt=\"Loss_function\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29beb5-4327-4bc4-bd82-e4de1328ae2b",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "How do we know the values to update the numbers in our weight and biases parameters? We use derivatives on the cost function and a lot of math. We want the cost function to be minimized, so we use gradient descent to find the lowest cost point.\n",
    "\n",
    "<img src=\"../media/intro/Gradient_Descent.jpg\" alt=\"Gradient_Descent\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd7904-6867-4587-a7ec-2a875bf1d28b",
   "metadata": {},
   "source": [
    "#### Activation function\n",
    "\n",
    "As it turns out, no matter how many neurons we use, if they are all linear functions, the end result if a linear function as well! This severely hinders our options. An activation function is non-linear, essentially allows us to say \"behave like this linear function, but only after a threshold.\" It's like adding a digital switch to an analog circuit, and it makes possible Neural Networks. A common activation function is the REctified Linuear Unit, ReLU: `y=x (if x>0)`\n",
    "\n",
    "<img src=\"../media/intro/ReLU.png\" alt=\"ReLU\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b4918-197e-4bf4-9393-68bdb35e2840",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "With each neuron having parameters (weight, bias) and an activation function (e.g, ReLU), we can make a Neural Network, NN. A shallow NN has one internal layer, and it's fully connected.\n",
    "\n",
    "Displayed is also a deep NN has multiple hidden layers.\n",
    "\n",
    "![](../media/intro/Neural_networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72292929-becf-4f13-8181-84f2e0893f24",
   "metadata": {},
   "source": [
    "#### Layers\n",
    "\n",
    "\n",
    "- The input layer is generally a flattened version of your data. For example, an linear array of each (R)ed, (G)reen, and (B)lue value for each pixel in an image.\n",
    "- The hidden layers are your Neural Network Layers\n",
    "- The output layer corresponds to how many outputs you need. One for binary classification, multiple for categorization, etc.\n",
    "\n",
    "Do you see the ReLU activation function above? It's unbounded and generally doesn't work well for outputs, since we want output ranges. For example, probability outputs should be between 0 and 1. We generally use the sigmoid function for the output layer instead.\n",
    "\n",
    "<img src=\"../media/intro/Sigmoid.png\" alt=\"Sigmoid\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fe3b1b7-027b-4dfd-9269-9d326a059910",
   "metadata": {},
   "source": [
    "#### Multi-class Classification\n",
    "\n",
    "When having multiple dependent outputs, for example in categorization, it's useful to use Softmax regression. It's an output layer showing the probabilities for an input to be in any category, with probabilities adding up to 1.\n",
    "\n",
    "\n",
    "<img src=\"../media/intro/Softmax_regression.png\" alt=\"Image by @tpreethi\" style=\"width: 40%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b193479-4faf-494a-839d-a9cac3efe7f0",
   "metadata": {},
   "source": [
    "### Forward and Backward Propagation\n",
    "\n",
    "We train NNs from scratch. We initialize our model with random weights and zero biases, and pass our training data through the NN to get predicted outputs. These predictions will probably suck. But from the training data we know what they *should* be, so we calculate the loss function on that. Then we calculate the gradients that would reduce the loss function. And then we use these gradient values to backpropagate changes into the NN, updating parameter values for weights and biases. And guess what? Our predictions will be better next time.\n",
    "\n",
    "<img src=\"../media/intro/backpropagation.png\" alt=\"backpropagation\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81877b-1ec6-4252-97cf-7567dcd355a7",
   "metadata": {},
   "source": [
    "### The AI model learning process\n",
    "\n",
    "- Do a forward pass\n",
    "- Calculate loss\n",
    "- Calculate gradients\n",
    "- Do a backward pass\n",
    "- Update parameters\n",
    "- Repeat multiple times (epochs) until we get close to expected results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fb90e-2e08-4ed4-9cec-3bdddac51308",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "To tune our NN, we need to choose hyperparamaters. These are manually set variables that control the training process of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada5b2a-cfae-4136-a258-dad81dd4b610",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "After we get the gradients from our loss function, we update the parameters by a fraction of the gradient to prevent overshoot. This fraction is the learning rate.\n",
    "\n",
    "<img src=\"../media/intro/LearningRate.png\" alt=\"LearningRate by jeremyjordan.me\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f98a3-2d1f-446d-ba8a-0c215737d403",
   "metadata": {},
   "source": [
    "### Train / Dev / Test\n",
    "\n",
    "You have a labeled dataset, but your model should work on new data as well. So, we split datasets into training set, development set, and test set.\n",
    "\n",
    "- **Train**: data used to train the model.\n",
    "- **Dev**: data used for hyperparameter tuning. Matches production data (e.g: traffic pictures from a specific webcam)\n",
    "- **Test**: This set is never seen during training, and is only used to determine accuracy. Matches production data.\n",
    "\n",
    "**Guideline:**\n",
    "- Decision Making: Train + Dev\n",
    "- Reflect world data: Dev + Test (taken from same distribution)\n",
    "\n",
    "The split between sets is a hyperparameter. Recommended split (with Large datasets are > 1 million):\n",
    "\n",
    "![](../media/intro/Datasets.png \"towardsdatascience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b433fa-2e6e-4f60-b121-4919aba569a0",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Many times we overfit our data to our training set, and it doesn't perform well on our test set or real world data. To avoid that, we use regularization. Examples:\n",
    "\n",
    "- **Normalizing inputs**: NNs perform better with average input values close to zero.\n",
    "- **Dropout**: If random neurons are ignored every now and then, the end result is not fixated on any specific feature.\n",
    "- **Data augmentation**: Modify your existing data, to expand your training set. For example, rotating/scaling/cropping your images.\n",
    "\n",
    "![](../media/intro/Regularization.png \"Akash Shastri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba001e8d-96ac-4085-8b93-109ed874d07f",
   "metadata": {},
   "source": [
    "### Optimization Algorithms\n",
    "\n",
    "We want to optimize our models to converge to a minimum loss quickly. For that we:\n",
    "\n",
    "- Use mini batches of training data, instead of waiting for the whole set to process.\n",
    "- Include momentum when doing gradient descent (RMSprop).\n",
    "- Also include acceleration when doing gradient descent (Adam).\n",
    "- Learning Rate decay: Start learning fast, then converge slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d080afb-c452-47a9-86eb-12b9686adc6d",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "When selecting hyperparameters, remember that the scale for the values is usually logarithmic, not linear.\n",
    "\n",
    "When tuning, work on a single parameter at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0edd1-09a4-44ee-9918-26a565d281b5",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "CNNs have revolutionized the field of computer vision. They are specifically designed to recognize visual patterns directly from pixel images with minimal preprocessing. CNNs are hierarchical models where neurons in one layer connect to neurons in the next layer in a limited fashion, somewhat like the receptive field in human vision.\n",
    "\n",
    "A typical CNN architecture consists of:\n",
    "\n",
    "- **Convolutional Layers**: Apply convolution operation on the input layer to detect features.\n",
    "- **Activation Layers**: Introduce non-linearity to the model (typically ReLU).\n",
    "- **Pooling Layers**: Perform down-sampling operations to reduce dimensionality.\n",
    "- **Fully Connected Layers**: After several convolutional and pooling layers, the high-level reasoning in the neural network happens via fully connected layers.\n",
    "\n",
    "![](../media/intro/CNN.png \"python.plainenglish.io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f66bc-b1b1-495c-9b89-a28aac549c28",
   "metadata": {},
   "source": [
    "### Residual Networks (ResNet)\n",
    "\n",
    "For most networks, a large number of inputs (e.g: pixels) get monotonically reduced to a small number of outputs (e.g: class). Spatial input information is lost in deeper layers. With residual networks, or ResNets, parts of previous layers are copied, or 'skipped' to deeper layers, providing greater context.\n",
    "\n",
    "\n",
    "<img src=\"../media/intro/SkipConnections.jpg\" alt=\"SkipConnections by analyticsvidhya\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf43ff-d710-4097-bfbc-744c5eeef953",
   "metadata": {},
   "source": [
    "#### Fine-tuning\n",
    "\n",
    "A useful trait of ResNets is that the offer transfer learning. Deep ResNets are trained on millions of images to classify a 1000 different classes (e.g: ResNet18). That means most of the layers can already detect useful image features.\n",
    "\n",
    "Given that ResNets can already detect useful features, we use fine-tuning to achieve transfer learning. We replace the existing output layer (or optionally, additional end layers) to fit our model outputs (for example, classifying between diferent types of apples). Training is fast since it only happens in the last layers.\n",
    "\n",
    "<img src=\"../media/intro/Fine-Tuning.png\" alt=\"Fine-Tuning by geeksforgeeks\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76165866-855d-4283-8e25-6d5bdd7e1666",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "You've learned about a lot of Deep Learning concepts. It's time to see them in action.\n",
    "\n",
    "The notebooks following this one will have you explore PyTorch, Datasets, Neural Networks, Regularization, CNNs, and ResNets. Click on them in the navigation panel and run the cells as you progress through the material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bb342-2d38-45db-bf4a-028d2bcaf826",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [pytorch-fashionMNIST-tutorial](https://github.com/junaidaliop/pytorch-fashionMNIST-tutorial/blob/main/pytorch_fashion_mnist_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8688cb-7a43-4dab-bc24-3d4905940a50",
   "metadata": {},
   "source": [
    "**Next Notebook: [01-Birds](01-Birds.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386a84a-d32b-442f-bc1e-bf25fce22145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
