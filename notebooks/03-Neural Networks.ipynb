{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd3a42efa31ff3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 03- Neural Networks\n",
    "\n",
    "This is an exploration for creating AI models, starting with a simple shallow Neural Network (NN). Neural networks forward pass input features through their inner layers, and then multiply them by weights and offset them by bias parameters. There is non-linearity activation functions added to each layer. The output can be a single value, like a binary classification system, or a combination of values. The comparison of expected vs actual data is sent as feedback through a backward pass, and the parameters updated through gradient descent to minimize the cost function, bringing the outputs closer to expected on later passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79643102-dee5-42bd-b7c6-7dd6d9dc2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e246ed2-a6e9-4440-aa97-8a4104e1d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16239a-058f-4fdd-b437-4555ebe0b7ff",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The data is labeled, and separated into train, validate, and test data. Train data will be used to compute inner layer parameters. Validate/Dev data helps with selecting better model hyperparameters and reducing variance. Test data approximates if the model will perform well with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83898166-5ad9-410c-b211-b43371db5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cbe20-f936-4958-ba44-e15ecc82a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()\n",
    "     #,transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "train_ds  = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_ds  = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f4d2c-ba89-4c98-9f8c-ba81f74bb818",
   "metadata": {},
   "source": [
    "### Split training set into training and validation\n",
    "\n",
    "For < 100K item datasets, generally 80% test, 20% dev split is good. For larger datasets, both dev and test ratios can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e9a95-762e-4c0e-b356-7dbec7498112",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO_VALIDATION = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b7ed9-e2f5-47e5-9020-099f5c7f62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = len(train_ds)\n",
    "indices = list(range(train_num))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(RATIO_VALIDATION * train_num))\n",
    "val_idx, train_idx = indices[:split], indices[split:]\n",
    "len(val_idx), len(train_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0796e-95b4-41c7-bdb6-8f9d08dac7d5",
   "metadata": {},
   "source": [
    "### Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445d10d-bada-463f-a00b-e5fdf7bb5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ebf94e-99f9-4cf7-9fb5-51342b06e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "val_dl   = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "test_dl  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c25cb9-5faf-4c8c-9dcb-ce2f45b4b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample\n",
    "image, label = next(iter(train_dl))\n",
    "print(image[0].shape, label.shape)\n",
    "classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']\n",
    "print(classes[label[0].item()])\n",
    "plt.imshow(image[0].numpy().squeeze(), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d0d70-c52e-491a-940d-cecb2fe9ba9a",
   "metadata": {},
   "source": [
    "## Shallow Neural Network\n",
    "\n",
    " A shallow NN is one that only has a single hidden layer of weights and biases applied to the input, then sent to the output after an activation function.\n",
    " \n",
    " The hidden layer here is a fully connected layer with a Rectified Linear Unit (RELU) non-linear activation.\n",
    " \n",
    " The output layer activation is a SoftMax function, generally used for categorical classifiers, where each output shows the scaled probability of the input matching that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9da7ec-0a6a-405a-a771-183ad57cfe6d",
   "metadata": {},
   "source": [
    "![](../media/neural_networks/Shallow_Neural_network.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126d78d-ac04-426c-b64b-d32da8e951eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are classifiying against 10 classes\n",
    "OUTPUTS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4530d69-6f05-43f0-ad6a-a298b393bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable is the number of parameters in the single inner layer of this shallow neural network.\n",
    "HIDDEN_PARAMETERS = 128\n",
    "\n",
    "input_features = image[0].shape[0] * image[0].shape[1] * image[0].shape[2] # Total features computed from input data: #color channels * width * height pixels of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96020686-a688-41d4-85f6-2c006ebfb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e571b1-d97f-4d67-8b1c-1b2cf7d7cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(OrderedDict([('fc1', nn.Linear(input_features, HIDDEN_PARAMETERS)), # Fully connected NN\n",
    "                                   ('relu1', nn.ReLU()), # Activation function\n",
    "                                   ('output', nn.Linear(HIDDEN_PARAMETERS, OUTPUTS)), # Fully connected NN\n",
    "                                   ('logsoftmax', nn.LogSoftmax(dim=1))])) # Softmax activation for categorization\n",
    "# Use GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8455bc-0151-4bbb-9d2f-eec28013d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08235be-1166-4fec-96bb-a8f040fef5c2",
   "metadata": {},
   "source": [
    "### Loss Function (Criterion) and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33eb373-1097-463f-9cee-e8062aa8e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a model hyperparameter. Large values can fail to minimize the cost function, small values might mean more time spent on iterations.\n",
    "LEARNING_RATE = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e7c54-1719-4d1f-bbcd-5169cde59ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our loss function\n",
    "# Cross Entropy Loss is the traditional loss function for neural networks.\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa98eab-cee0-4c1f-a76a-dd6b823c5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our optimizer\n",
    "# Stochastic Gradient Descent is the traditional optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190278c5-23f5-4463-8ab0-8e12ab071738",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "### Training Loop (Explained)\n",
    "\n",
    "Training a neural network involves iteratively updating its weights to minimize the loss function. This process is typically achieved using gradient descent optimization algorithms. Here's an in-depth explanation of the training loop:\n",
    "\n",
    "1. **Epochs**: An epoch represents one complete forward and backward pass of all the training examples. The number of epochs (`n_epochs`) is the number of times the learning algorithm will work through the entire training dataset. Usually a custom hyperparameter.\n",
    "\n",
    "2. **Model Training Mode**: Neural networks can operate in different modes - training and evaluation. Some layers, like dropout, behave differently in these modes. Setting the model to training mode ensures that layers like dropout function correctly.\n",
    "\n",
    "3. **Batch Processing**: Instead of updating weights after every training example (stochastic gradient descent) or after the entire dataset (batch gradient descent), we often update weights after a set of training examples known as a batch.\n",
    "\n",
    "4. **Zeroing Gradients**: In PyTorch, gradients accumulate by default. Before calculating the new gradients in the current batch, we need to set the previous gradients to zero.\n",
    "\n",
    "5. **Forward Pass**: The input data (images) are passed through the network, layer by layer, until we get the output. This process is called the forward pass.\n",
    "\n",
    "6. **Calculate Loss**: Once we have the network's predictions (outputs), we compare them to the true labels using a loss function. This gives a measure of how well the network's predictions match the actual labels.\n",
    "\n",
    "7. **Backward Pass**: To update the weights, we need to know the gradient of the loss function with respect to each weight. The backward pass computes these gradients.\n",
    "\n",
    "8. **Update Weights**: The optimizer updates the weights based on the gradients computed in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4a038-4b5a-45f1-9a20-227f8fcf8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model, with parameterized loss function and optimizer\n",
    "def train_validate(model, loss_fn, optimizer, trainloader, testloader, device, n_epochs=25):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set mode to training - Dropouts will be used here\n",
    "        train_epoch_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # flatten the images to batch_size x 784\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            # backpropogation\n",
    "            train_batch_loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_loss.backward()\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += train_batch_loss.item()\n",
    "        # One epoch of training complete\n",
    "        # calculate average training epoch loss\n",
    "        train_epoch_loss = train_epoch_loss/len(trainloader)\n",
    "\n",
    "        # Now Validate on testset\n",
    "        with torch.no_grad():\n",
    "            test_epoch_acc = 0\n",
    "            test_epoch_loss = 0\n",
    "            model.eval() # Set mode to eval - Dropouts will NOT be used here\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)                    \n",
    "                # flatten images to batch_size x 784\n",
    "                images = images.view(images.shape[0], -1)\n",
    "                # make predictions \n",
    "                test_outputs = model(images)\n",
    "                # calculate test loss\n",
    "                test_batch_loss = loss_fn(test_outputs, labels)\n",
    "                test_epoch_loss += test_batch_loss\n",
    "                \n",
    "                # get probabilities, extract the class associated with highest probability\n",
    "                proba = torch.exp(test_outputs)\n",
    "                _, pred_labels = proba.topk(1, dim=1)\n",
    "                \n",
    "                # compare actual labels and predicted labels\n",
    "                result = pred_labels == labels.view(pred_labels.shape)\n",
    "                batch_acc = torch.mean(result.type(torch.FloatTensor))\n",
    "                test_epoch_acc += batch_acc.item()\n",
    "            # One epoch of training and validation done\n",
    "            # calculate average testing epoch loss\n",
    "            test_epoch_loss = test_epoch_loss/len(testloader)\n",
    "            test_epoch_loss = test_epoch_loss.cpu() # To be able to plot it\n",
    "            # calculate accuracy as correct_pred/total_samples\n",
    "            test_epoch_acc = test_epoch_acc/len(testloader)\n",
    "            # save epoch losses for plotting\n",
    "            train_losses.append(train_epoch_loss)\n",
    "            test_losses.append(test_epoch_loss)\n",
    "            # print stats for this epoch\n",
    "            print(f'Epoch: {epoch:02} -> train_loss: {train_epoch_loss:.10f}, val_loss: {test_epoch_loss:.10f}, ',\n",
    "                  f'val_acc: {test_epoch_acc*100:.2f}%')\n",
    "    # Finally plot losses\n",
    "    plt.plot(train_losses, label='train-loss')\n",
    "    plt.plot(test_losses, label='val-loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c25e8-4ae3-4623-bf5f-9b44827de8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to run\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5ee3b-5faa-44f6-a9ab-a8e76b867fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate\n",
    "train_validate(model, loss_fn, optimizer, train_dl, val_dl, device, n_epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf5720-685e-454c-a82d-882fc2a41cba",
   "metadata": {},
   "source": [
    "### Validate on test set\n",
    "\n",
    "Once our model is trained, it's crucial to evaluate its performance on unseen data. We'll:\n",
    "\n",
    "1. Generate predictions for the test set.\n",
    "2. Compute the overall accuracy.\n",
    "3. Examine the model's performance in detail using a confusion matrix and classification report.\n",
    "\n",
    "These tools will provide insights into specific areas where the model excels or might need improvement.\n",
    "\n",
    "Note: We don't want to compute gradients, so we use `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef31895-ae48-4079-8f26-977ceb9d03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "def test_validate(model, test_dl, device):\n",
    "    with torch.no_grad(): # Ne need to calculate backward pass for test set.\n",
    "        batch_acc = []\n",
    "        model.eval()\n",
    "        for images, labels in test_dl:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # flatten images to batch_size x 784\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            # make predictions and get probabilities\n",
    "            proba = torch.exp(model(images))\n",
    "            # extract the class associted with highest probability\n",
    "            _, pred_labels = proba.topk(1, dim=1)\n",
    "            # compare actual labels and predicted labels\n",
    "            result = pred_labels == labels.view(pred_labels.shape)\n",
    "            acc = torch.mean(result.type(torch.FloatTensor))\n",
    "            batch_acc.append(acc.item())\n",
    "        else:\n",
    "            print(f'Test Accuracy: {torch.mean(torch.tensor(batch_acc))*100:.2f}%')\n",
    "        return batch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c17d20-4fe2-44a2-bfb1-c2979bf6eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validate(model, test_dl, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5fb9b-d522-4a48-91f0-29c3613a0e43",
   "metadata": {},
   "source": [
    "## Shallow NN Hyperparameters\n",
    "\n",
    "We have some `PARAMETERS` that can be changed to see if we can get better and faster results. Lets change some of them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6efdb-458f-450b-82e8-227cbb053d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume we will change our loss function and optimizer to better suit our classification problem. If that's true, we could accelerate our learning rate to get faster training\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f7ba7-56e5-4aac-bec4-78d69663f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our loss function\n",
    "# Negative log likelihood loss, useful to train a classification problem.\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820911c-b9bc-4a48-a590-91899dd44039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our optimizer\n",
    "# Adam (Adaptive Moment Estimation) optimizer. It is a robust method that builds momentum to speed up training, and accelerating in the right directions based on previous history.\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1418250-2e8d-4fb1-aa4f-44837fe478e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And because we might train faster, we assume we'll need less iterations\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f37f5-0406-4812-91d1-66ddc23567e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with new parameters\n",
    "train_validate(model, loss_fn, optimizer, train_dl, val_dl, device, n_epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43dba84-8c99-4b1a-9315-92a98086fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validate(model, test_dl, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822332c-16f0-4009-9c39-2c49afc16023",
   "metadata": {},
   "source": [
    "We increased our learning rate by 3, and could reduce our number of epochs by 40%, but the learning is showing some diverging instead of converging characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d04439-b5ce-49f6-86a1-6263ec7c3c6e",
   "metadata": {},
   "source": [
    "### Hidden parameters\n",
    "\n",
    "The number of parameters in the hidden layer can also change, and might provide different results. We'll reduce them by half. The test set accuracy will probably hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6878d577-f809-40c7-b873-9bbf5adbba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_PARAMETERS = 64 # Decrease by factor of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111290cb-0857-4c26-ac21-eb994f43ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(OrderedDict([('fc1', nn.Linear(input_features, HIDDEN_PARAMETERS)),\n",
    "                                   ('relu1', nn.ReLU()),\n",
    "                                   ('output', nn.Linear(HIDDEN_PARAMETERS, OUTPUTS)),\n",
    "                                   ('logsoftmax', nn.LogSoftmax(dim=1))]))\n",
    "# Use GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c43ca7-4321-4dc5-aec2-3bc6c657f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7457a5-23a2-4104-9b5b-70a40c8eca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate(model, loss_fn, optimizer, train_dl, val_dl, device, n_epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d477af-6e40-4d8b-999e-d3fd1ffc31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validate(model, test_dl, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e73d51-faef-4d32-9924-d8251ca99ebd",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "\n",
    "Deep Neural Networks contain more than 1 hidden layer of parameters. Each of these layers is usually a linear transformation with weights and a bias, coupled with a non-linear activation function. Let's redefine our model with two layers, and keep all other hyperparameters the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65949afd-65c8-4f4b-978e-fb27335176e4",
   "metadata": {},
   "source": [
    "![](../media/neural_networks/Deep_Neural_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc59fa10-4c7a-4739-97b2-77ef49119859",
   "metadata": {},
   "source": [
    "### Two layer DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb4782b-b1dc-49f5-8936-972a5d91b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_PARAMETERS = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a16646-5caf-4d4f-9892-386fb19aa9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(OrderedDict([('fc1', nn.Linear(input_features, HIDDEN_LAYER_PARAMETERS[0])),\n",
    "                                   ('relu1', nn.ReLU()),\n",
    "                                   ('fc2', nn.Linear(HIDDEN_LAYER_PARAMETERS[0], HIDDEN_LAYER_PARAMETERS[1])),\n",
    "                                   ('relu2', nn.ReLU()),\n",
    "                                   ('output', nn.Linear(HIDDEN_LAYER_PARAMETERS[1], OUTPUTS)),\n",
    "                                   ('logsoftmax', nn.LogSoftmax(dim=1))]))\n",
    "# Use GPU if available\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea38930-f661-45b8-ade5-a98e7256ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bddd7-a65f-4a5b-8f57-4286e4c806a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate(model, loss_fn, optimizer, train_dl, val_dl, device, n_epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc388bc2-02fc-41ec-937e-a7b822ae1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validate(model, test_dl, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961cfdd-445b-4f7c-b43d-7ebef25d29ae",
   "metadata": {},
   "source": [
    "### Three layer DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db98f74-4e60-4073-889c-57e6efff9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_PARAMETERS = [64, 48, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917ba71-b473-4bc2-955a-662409178e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(OrderedDict([('fc1', nn.Linear(input_features, HIDDEN_LAYER_PARAMETERS[0])),\n",
    "                                   ('relu1', nn.ReLU()),\n",
    "                                   ('fc2', nn.Linear(HIDDEN_LAYER_PARAMETERS[0], HIDDEN_LAYER_PARAMETERS[1])),\n",
    "                                   ('relu2', nn.ReLU()),\n",
    "                                   ('fc3', nn.Linear(HIDDEN_LAYER_PARAMETERS[1], HIDDEN_LAYER_PARAMETERS[2])),\n",
    "                                   ('relu3', nn.ReLU()),\n",
    "                                   ('output', nn.Linear(HIDDEN_LAYER_PARAMETERS[2], OUTPUTS)),\n",
    "                                   ('logsoftmax', nn.LogSoftmax(dim=1))]))\n",
    "# Use GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a687f0-c3e0-47d8-81de-d8522ab56e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85497bf-80bd-45f6-96a8-4700d066fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate(model, loss_fn, optimizer, train_dl, val_dl, device, n_epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c3eda-1719-4747-82c9-a97db225fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validate(model, test_dl, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b08f2f-bf1c-422d-9631-0a2cd6b0cf68",
   "metadata": {},
   "source": [
    "We can modify our NN further or train it for longer, but you can probably see that our validation loss has a lot of variance, and that our test set underperforms in comparison. We're overfitting our data. Next, we'll take some measures to prevent that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1f881-9928-4034-aa99-7d48c3b5df16",
   "metadata": {},
   "source": [
    "**Next Notebook: [04-Regularization](04-Regularization.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7959475-04c1-4073-83cd-38186a2a9dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
