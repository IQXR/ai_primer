{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd3a42efa31ff3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 05- Convolutional Neural Networks\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision. They are specifically designed to recognize visual patterns directly from pixel images with minimal preprocessing. CNNs are hierarchical models where neurons in one layer connect to neurons in the next layer in a limited fashion, somewhat like the receptive field in human vision.\n",
    "\n",
    "CNNs are useful for finding patterns in images to recognize objects, classes, and categories.\n",
    "\n",
    "The first few layers recognize simple visual features, like edges. Deeper layers use the initial ones to build more sophisticated recognition patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6385cad-c14c-4853-8c59-7aa5c5a15362",
   "metadata": {},
   "source": [
    "![](../media/cnn/CNN_Activation-maximization.png \"International Journal of Computer Vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83898166-5ad9-410c-b211-b43371db5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e246ed2-a6e9-4440-aa97-8a4104e1d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a00a8-aedd-425a-b066-0cb013338d8f",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657ffb3-c9d5-449c-bbcd-35dab73d9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO_VALIDATION = 0.2\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c9bd8-d5c7-407b-b766-be0f7937f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model tools\n",
    "from scripts.model_tools import train_validate, test_validate, test_validate_confusion, set_fashion_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70222b1-c61c-4582-bf93-a746d472437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation pipeline. \n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds, test_ds, train_dl, val_dl, test_dl, classes = set_fashion_dataset(transform, RATIO_VALIDATION, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0dfb9e-95a7-467b-a951-35e2e4a8c999",
   "metadata": {},
   "source": [
    "A typical CNN architecture consists of:\n",
    "\n",
    " 1. Convolutional Layers: Apply convolution operation on the input layer to detect features.\n",
    " 2. Activation Layers: Introduce non-linearity to the model (typically ReLU).\n",
    " 3. Pooling Layers: Perform down-sampling operations to reduce dimensionality.\n",
    " 4. Fully Connected Layers: After several convolutional and pooling layers, the high-level reasoning in the neural network happens via fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6dd19-a70f-4204-9b14-75a3fcd344e8",
   "metadata": {},
   "source": [
    "![](../media/intro/CNN.png \"python.plainenglish.io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0aef7-bc4a-47e7-933b-4c2a154548ca",
   "metadata": {},
   "source": [
    "Let's design a basic CNN for our dataset. Instead of defining the model as a sequential setup of layers with `nn.Sequential`, we will extend `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804ebb8-df4f-446e-ab4b-fdd0f2aa57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        # Input: [batch_size, 1, 28, 28]\n",
    "        # Convolution 1 setup:  In-channels:1, Out_channels: 32, kernel_size: 3\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)  #,  Output: [batch_size, 32, 26, 26]. Because of kernel size with no padding, output shape is smaller\n",
    "        \n",
    "        # Input: [batch_size, 32, 26, 26]\n",
    "        # Convolution 2 setup:  In-channels:32, Out_channels: 64, kernel_size: 3\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3) # Output: [batch_size, 64, 11, 11]. Output was halved first, because of maxpool size 2\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)  # Flattening: [batch_size, 64*5*5]\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [batch_size, 1, 28, 28]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Shape: [batch_size, 32, 26, 26]\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Shape: [batch_size, 32, 13, 13]\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Shape: [batch_size, 64, 11, 11]\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Shape: [batch_size, 64, 5, 5]\n",
    "        \n",
    "        x = x.view(-1, 64 * 5 * 5) # Flattening\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6d64e-a7ef-482a-acfe-64823a9456a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22a48f-d45c-4c7d-a478-851da6780d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211f3ca-6be1-4e1e-846a-710873277e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate(model, criterion, optimizer, train_dl, val_dl, device, n_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b30c8-daac-4a3c-893f-440ccd8db8ad",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's do a more sofisticated evaluation with a classification report and confusion matrix. We will gain more insights into our data, instead of just relying on a single accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc799f1-ab48-4733-84a7-abfcc821fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries:\n",
    "# numpy for numerical operations\n",
    "# sklearn.metrics for evaluation metrics like classification report and confusion matrix\n",
    "# seaborn and matplotlib for data visualization\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the model to evaluation mode. This is important as certain layers like dropout behave differently during training and evaluation.\n",
    "model.eval()\n",
    "\n",
    "# Lists to store all predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# We don't want to compute gradients during evaluation, hence wrap the code inside torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    batch_acc = []\n",
    "    # Iterate over all batches in the test loader\n",
    "    for images, labels in train_dl:\n",
    "        # Transfer images and labels to the computational device (either CPU or GPU)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Pass the images through the model to get predictions\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get the class with the maximum probability as the predicted class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Extend the all_preds list with predictions from this batch\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        # Extend the all_labels list with true labels from this batch\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compare actual labels and predicted labels\n",
    "        result = predicted == labels.view(predicted.shape)\n",
    "        acc = torch.mean(result.type(torch.FloatTensor))\n",
    "        batch_acc.append(acc.item())\n",
    "\n",
    "# Print a classification report which provides an overview of the model's performance for each class\n",
    "print(classification_report(all_labels, all_preds, target_names=classes))\n",
    "\n",
    "# Compute the confusion matrix using true labels and predictions\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Visualize the confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted Label')  # x-axis label\n",
    "plt.ylabel('True Label')       # y-axis label\n",
    "plt.title('Confusion Matrix')  # Title of the plot\n",
    "plt.show()                     # Display the plot\n",
    "\n",
    "print(f'Test Accuracy: {torch.mean(torch.tensor(batch_acc))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ad526-e69b-48b3-a92c-663eb5cebb04",
   "metadata": {},
   "source": [
    "The true label axis shows the actual category of the samples, and the predicted label comes from our model. The above matrix shows which samples were miscategorized. The tendency is that items that look similar, like pullovers and coats, or shirts and T-shirts, might be mispredicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cb8cb-873a-454f-b1a6-55d1d976cecf",
   "metadata": {},
   "source": [
    "## Regularization with dropout\n",
    "\n",
    "Let's use dropout layers to prevent the model from becoming too reliant on any specific neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1f57c-d4f3-4b5c-a995-08944450ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa613c-79b4-49eb-87f7-f7af8f896d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetDropout, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # Output shape: [batch_size, 32, 28, 28]\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) # Output shape: [batch_size, 64, 14, 14]\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) # Reduces spatial dimensions by half\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.25)  # Helps prevent overfitting\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)  # Flattened input to 512 output features\n",
    "        self.fc2 = nn.Linear(512, 10)          # 512 input features to 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # Shape: [batch_size, 32, 14, 14]\n",
    "        x = self.pool(F.relu(self.conv2(x))) # Shape: [batch_size, 64, 7, 7]\n",
    "        \n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
    "        x = self.dropout(x)         # Apply dropout\n",
    "        \n",
    "        x = F.relu(self.fc1(x))     # First fully connected layer with ReLU activation\n",
    "        x = self.fc2(x)             # Second fully connected layer\n",
    "        \n",
    "        # Here, we're not applying log_softmax. If you use nn.CrossEntropyLoss as the loss function later,\n",
    "        # it will implicitly apply softmax for you.\n",
    "        # If you plan on using nn.NLLLoss, uncomment the line below:\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model with dropout\n",
    "model_dropout = NetDropout().to(device)\n",
    "model_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68c812-1af8-47c7-8cec-a4cf56d455a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_dropout.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83ac3f-a377-469f-81ab-422eba66b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate(model_dropout, criterion, optimizer, train_dl, val_dl, device, n_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c20bb-ae8d-43b8-bcb7-94ee14dc8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validate_confusion(model_dropout, val_dl, device, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805368db-0f33-46d0-bf5e-9b099b1fa9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate against test set\n",
    "test_validate_confusion(model_dropout, test_dl, device, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f47f7-504e-47e8-a5b4-c9b9563535b9",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "We can add some data augmentation here as well. From the results in the Deep Neural Network notebook, we don't expect much benefit, so we'll skip it for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae4ca2-1e65-454a-9046-ff07e9c62a9d",
   "metadata": {},
   "source": [
    "**Next Notebook: [06-Resnet](06-Resnet.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee119f-b0e2-49d2-9e87-6f355fffdc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
