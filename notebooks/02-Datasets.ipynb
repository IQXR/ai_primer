{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd3a42efa31ff3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 02 - Datasets\n",
    "\n",
    "For any AI task, we need a dataset, that maps inputs to labeled outputs. With this information, we can train a model on this dataset to predict results on new, unseen data.\n",
    "\n",
    "We will use PyTorch to work on this dataset.\n",
    "\n",
    "(If you want a comprehensive, executable reference for Pytorch, you can go to the [PyTorch Tutorial](A0-PyTorch%20Tutorial.ipynb) notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1734b8-e492-43a4-b741-1409109413cd",
   "metadata": {},
   "source": [
    "### Understanding the Fashion MNIST Dataset\n",
    "\n",
    "The Fashion MNIST dataset is a collection of grayscale images of 10 fashion categories, each of size 28x28 pixels. It's used as a drop-in replacement for the classic MNIST dataset. It serves as a more challenging classification problem than the regular MNIST digit dataset due to the similarities in clothing items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b2fdb-6762-4e4c-a891-cd63ee25b376",
   "metadata": {},
   "source": [
    "![](../media/datasets/FashionMNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3fec2-35bf-4f37-8025-ce0e9b5302f6",
   "metadata": {},
   "source": [
    "Each image in the dataset corresponds to a label from 0-9, representing the ten categories:\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d800512-321d-45c0-b6ce-7f541913e5b9",
   "metadata": {},
   "source": [
    "In this tutorial, we are primarily using `torchvision` to access the Fashion MNIST dataset and apply transformations to the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be8a59-ccfa-40b8-bf21-188274b91299",
   "metadata": {},
   "source": [
    "### Loading a Dataset\n",
    "\n",
    "We need to import some libraries. `torch` for PyTorch, and `matplotlib` for plotting figures. These were installed for you with Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e216062-8750-4449-896f-7047133c8b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36286b69-70f3-4ae2-9b81-f54c6964c16b",
   "metadata": {},
   "source": [
    "Download the Fashion dataset, if it hasn't been downloaded already. Separate  into training and test data. We don't use a dev set, because we won't be modifying hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d0f8b-9a11-4e59-9194-0f5cebc1da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da90a43-1129-4b08-b543-3e23554fe7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13707c9f-2718-43d4-b080-4ec88c120aca",
   "metadata": {},
   "source": [
    "### Iterating and Visualizing the Dataset\n",
    "\n",
    "The dataset is a 10-class classifier. Let's plot some of its images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7d685-bbad-45ec-9caf-dca5a35f594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfea889-8976-454d-8b86-35bb74458a90",
   "metadata": {},
   "source": [
    "### Preparing your data for training with DataLoaders\n",
    "\n",
    "To simplify loading data from the dataset and into the processing stage, we use dataloaders. We can specify batch sizes here. Small batch sizes help fitting portions of the dataset into limited hardware (for example, a graphics card with limited VRAM). The selected batch size is 64. This is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4fe56-46ea-40d7-b1d3-3e044a3fc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577daf35-5e90-4969-a885-c71fce40b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the DataLoader\n",
    "\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}: {labels_map[int(label)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b17916-ae6e-4606-a82e-ff99031362b8",
   "metadata": {},
   "source": [
    "### Iterate\n",
    "\n",
    "Remember we told you you can execute a cell multiple times? Well, you can execute the cell above to go display each entry of the training dataset batch if you want.\n",
    "\n",
    "### Shapes\n",
    "\n",
    "Most AI models expect a specific input shape. In this case, `[64, 1, 28, 28]` means that we have 64 images in this batch, each image has one channel (B/W, color images usually have 3), and the size of the image is 28 pixels high by 28 pixels wide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38acaf0f-af7f-47cc-8ad1-2f944aa7f1a9",
   "metadata": {},
   "source": [
    "**Next Notebook: [03-Neural Networks](03-Neural%20Networks.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e62a73-e825-4dd5-993c-161484dd9e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
